# llm_adapters.py
# -*- coding: utf-8 -*-
import logging
from typing import Optional
from langchain_openai import ChatOpenAI, AzureChatOpenAI
import google.generativeai as genai
from azure.ai.inference import ChatCompletionsClient
from azure.core.credentials import AzureKeyCredential
from azure.ai.inference.models import SystemMessage, UserMessage
from openai import OpenAI
import requests
import os
import socket
import time


def check_base_url(url: str) -> str:
    """
    å¤„ç†base_urlçš„è§„åˆ™ï¼š
    1. å¦‚æœurlä»¥#ç»“å°¾ï¼Œåˆ™ç§»é™¤#å¹¶ç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„url
    2. å¦åˆ™æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ /v1åç¼€
    """
    import re
    url = url.strip()
    if not url:
        return url
        
    if url.endswith('#'):
        return url.rstrip('#')
        
    if not re.search(r'/v\d+$', url):
        if '/v1' not in url:
            url = url.rstrip('/') + '/v1'
    return url

def detect_and_setup_proxy():
    """
    æ£€æµ‹å¹¶è®¾ç½®ä»£ç†é…ç½®ï¼Œä¸»è¦é’ˆå¯¹Clashç­‰ä»£ç†å·¥å…·
    """
    # å¸¸è§çš„ä»£ç†ç«¯å£é…ç½®
    proxy_configs = [
        {"http": "http://127.0.0.1:7890", "https": "http://127.0.0.1:7890"},  # Clash for Windows é»˜è®¤HTTPä»£ç†
        {"http": "socks5://127.0.0.1:7891", "https": "socks5://127.0.0.1:7891"},  # Clash for Windows é»˜è®¤SOCKS5ä»£ç†
        {"http": "http://127.0.0.1:8080", "https": "http://127.0.0.1:8080"},  # å…¶ä»–å¸¸è§HTTPä»£ç†
        {"http": "http://127.0.0.1:1080", "https": "http://127.0.0.1:1080"},  # å…¶ä»–å¸¸è§ä»£ç†
    ]
    
    # æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­æ˜¯å¦å·²ç»è®¾ç½®äº†ä»£ç†
    if os.environ.get('HTTP_PROXY') or os.environ.get('HTTPS_PROXY'):
        logging.info("ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­å·²é…ç½®ä»£ç†")
        return True
    
    # å°è¯•æ£€æµ‹å¯ç”¨çš„ä»£ç†
    for proxy_config in proxy_configs:
        try:
            # è§£æä»£ç†åœ°å€å’Œç«¯å£
            import re
            http_proxy = proxy_config["http"]
            if "://" in http_proxy:
                proxy_url = http_proxy.split("://")[1]
            else:
                proxy_url = http_proxy
            
            if ":" in proxy_url:
                host, port = proxy_url.split(":")
                port = int(port)
                
                # æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(1)
                result = sock.connect_ex((host, port))
                sock.close()
                
                if result == 0:
                    # ç«¯å£å¼€æ”¾ï¼Œè®¾ç½®ä»£ç†
                    os.environ['HTTP_PROXY'] = proxy_config["http"]
                    os.environ['HTTPS_PROXY'] = proxy_config["https"]
                    os.environ['http_proxy'] = proxy_config["http"]  # å°å†™ç‰ˆæœ¬ï¼ŒæŸäº›åº“éœ€è¦
                    os.environ['https_proxy'] = proxy_config["https"]
                    logging.info(f"æˆåŠŸæ£€æµ‹å¹¶è®¾ç½®ä»£ç†: {proxy_config['http']}")
                    return True
                    
        except Exception as e:
            logging.debug(f"æ£€æµ‹ä»£ç† {proxy_config} å¤±è´¥: {e}")
            continue
    
    logging.warning("æœªæ£€æµ‹åˆ°å¯ç”¨çš„ä»£ç†é…ç½®ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨è®¾ç½®")
    return False

def test_google_connectivity():
    """
    æµ‹è¯•èƒ½å¦è®¿é—®GoogleæœåŠ¡
    """
    try:
        import urllib.request
        # å°è¯•è®¿é—®Google AIçš„endpoint
        response = urllib.request.urlopen('https://generativelanguage.googleapis.com', timeout=10)
        return response.getcode() == 200
    except Exception as e:
        logging.debug(f"Googleè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

class BaseLLMAdapter:
    """
    ç»Ÿä¸€çš„ LLM æ¥å£åŸºç±»ï¼Œä¸ºä¸åŒåç«¯ï¼ˆOpenAIã€Ollamaã€ML Studioã€Geminiç­‰ï¼‰æä¾›ä¸€è‡´çš„æ–¹æ³•ç­¾åã€‚
    """
    def invoke(self, prompt: str) -> str:
        raise NotImplementedError("Subclasses must implement .invoke(prompt) method.")

class DeepSeekAdapter(BaseLLMAdapter):
    """
    é€‚é…å®˜æ–¹/OpenAIå…¼å®¹æ¥å£ï¼ˆä½¿ç”¨ langchain.ChatOpenAIï¼‰
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = ChatOpenAI(
            model=self.model_name,
            api_key=self.api_key,
            base_url=self.base_url,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        response = self._client.invoke(prompt)
        if not response:
            logging.warning("No response from DeepSeekAdapter.")
            return ""
        return response.content

class OpenAIAdapter(BaseLLMAdapter):
    """
    é€‚é…å®˜æ–¹/OpenAIå…¼å®¹æ¥å£ï¼ˆä½¿ç”¨ langchain.ChatOpenAIï¼‰
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = ChatOpenAI(
            model=self.model_name,
            api_key=self.api_key,
            base_url=self.base_url,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        response = self._client.invoke(prompt)
        if not response:
            logging.warning("No response from OpenAIAdapter.")
            return ""
        return response.content

class GeminiAdapter(BaseLLMAdapter):
    """
    é€‚é… Google Gemini (Google Generative AI) æ¥å£
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout
        
        # æ£€æµ‹å¹¶è®¾ç½®ä»£ç†ï¼ˆé’ˆå¯¹ä¸­å›½åœ°åŒºç½‘ç»œè®¿é—®é—®é¢˜ï¼‰
        try:
            # é¦–å…ˆæµ‹è¯•ç›´è¿
            if not test_google_connectivity():
                logging.info("ç›´è¿GoogleæœåŠ¡å¤±è´¥ï¼Œå°è¯•æ£€æµ‹å¹¶è®¾ç½®ä»£ç†...")
                proxy_detected = detect_and_setup_proxy()
                if proxy_detected:
                    # ç­‰å¾…ä¸€ä¸‹è®©ä»£ç†è®¾ç½®ç”Ÿæ•ˆ
                    time.sleep(1)
                    if test_google_connectivity():
                        logging.info("âœ… ä»£ç†è®¾ç½®æˆåŠŸï¼Œå¯ä»¥è®¿é—®GoogleæœåŠ¡")
                    else:
                        logging.warning("âš ï¸ ä»£ç†å·²è®¾ç½®ä½†ä»æ— æ³•è®¿é—®GoogleæœåŠ¡ï¼Œè¯·æ£€æŸ¥ä»£ç†é…ç½®")
                else:
                    logging.warning("âš ï¸ æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°ä»£ç†ï¼Œå¦‚æœåœ¨ä¸­å›½åœ°åŒºä½¿ç”¨ï¼Œè¯·ç¡®ä¿VPN/ä»£ç†æ­£å¸¸è¿è¡Œ")
            else:
                logging.info("âœ… å¯ä»¥ç›´æ¥è®¿é—®GoogleæœåŠ¡")
        except Exception as e:
            logging.warning(f"ä»£ç†æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        
        # é…ç½®APIå¯†é’¥
        genai.configure(api_key=self.api_key)
        
        # å¦‚æœæä¾›äº†base_urlä¸”ä¸ä¸ºç©ºï¼Œåˆ™é…ç½®è‡ªå®šä¹‰endpoint
        if base_url and base_url.strip():
            # è®¾ç½®ç¯å¢ƒå˜é‡æ¥ä½¿ç”¨è‡ªå®šä¹‰endpoint
            os.environ['GOOGLE_AI_STUDIO_API_ENDPOINT'] = base_url.strip()
        
        # åˆ›å»ºç”Ÿæˆæ¨¡å‹
        self._model = genai.GenerativeModel(model_name=self.model_name)

    def invoke(self, prompt: str) -> str:
        try:
            # åˆ›å»ºç”Ÿæˆé…ç½®
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=self.max_tokens,
                temperature=self.temperature,
            )
            
            # ç”Ÿæˆå†…å®¹
            response = self._model.generate_content(
                contents=prompt,
                generation_config=generation_config,
                request_options={"timeout": self.timeout} if self.timeout else None
            )
            
            if response and response.text:
                return response.text
            else:
                logging.warning("No text response from Gemini API.")
                return ""
                
        except Exception as e:
            error_msg = str(e)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é—®é¢˜
            if any(keyword in error_msg.lower() for keyword in 
                   ['connection', 'timeout', 'network', 'proxy', 'ssl', 'certificate']):
                logging.error(f"Gemini API ç½‘ç»œè¿æ¥å¤±è´¥: {e}")
                logging.info("ğŸ’¡ å¦‚æœæ‚¨åœ¨ä¸­å›½åœ°åŒºï¼Œè¯·ç¡®ä¿:")
                logging.info("   1. VPN/ä»£ç†æœåŠ¡æ­£å¸¸è¿è¡Œï¼ˆå¦‚Clash for Windowsï¼‰")
                logging.info("   2. ä»£ç†ç«¯å£7890(HTTP)æˆ–7891(SOCKS5)å¯è®¿é—®")
                logging.info("   3. å¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ­£å¸¸è®¿é—®Google")
                
                # å°è¯•é‡æ–°æ£€æµ‹ä»£ç†
                if not test_google_connectivity():
                    logging.info("ğŸ”„ æ­£åœ¨é‡æ–°å°è¯•ä»£ç†æ£€æµ‹...")
                    detect_and_setup_proxy()
            else:
                logging.error(f"Gemini API è°ƒç”¨å¤±è´¥: {e}")
            
            return ""

class AzureOpenAIAdapter(BaseLLMAdapter):
    """
    é€‚é… Azure OpenAI æ¥å£ï¼ˆä½¿ç”¨ langchain.ChatOpenAIï¼‰
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        import re
        match = re.match(r'https://(.+?)/openai/deployments/(.+?)/chat/completions\?api-version=(.+)', base_url)
        if match:
            self.azure_endpoint = f"https://{match.group(1)}"
            self.azure_deployment = match.group(2)
            self.api_version = match.group(3)
        else:
            raise ValueError("Invalid Azure OpenAI base_url format")
        
        self.api_key = api_key
        self.model_name = self.azure_deployment
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = AzureChatOpenAI(
            azure_endpoint=self.azure_endpoint,
            azure_deployment=self.azure_deployment,
            api_version=self.api_version,
            api_key=self.api_key,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        response = self._client.invoke(prompt)
        if not response:
            logging.warning("No response from AzureOpenAIAdapter.")
            return ""
        return response.content

class OllamaAdapter(BaseLLMAdapter):
    """
    Ollama åŒæ ·æœ‰ä¸€ä¸ª OpenAI-like /v1/chat æ¥å£ï¼Œå¯ç›´æ¥ä½¿ç”¨ ChatOpenAIã€‚
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        if self.api_key == '':
            self.api_key= 'ollama'

        self._client = ChatOpenAI(
            model=self.model_name,
            api_key=self.api_key,
            base_url=self.base_url,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        response = self._client.invoke(prompt)
        if not response:
            logging.warning("No response from OllamaAdapter.")
            return ""
        return response.content

class MLStudioAdapter(BaseLLMAdapter):
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = ChatOpenAI(
            model=self.model_name,
            api_key=self.api_key,
            base_url=self.base_url,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.invoke(prompt)
            if not response:
                logging.warning("No response from MLStudioAdapter.")
                return ""
            return response.content
        except Exception as e:
            logging.error(f"ML Studio API è°ƒç”¨è¶…æ—¶æˆ–å¤±è´¥: {e}")
            return ""

class AzureAIAdapter(BaseLLMAdapter):
    """
    é€‚é… Azure AI Inference æ¥å£ï¼Œç”¨äºè®¿é—®Azure AIæœåŠ¡éƒ¨ç½²çš„æ¨¡å‹
    ä½¿ç”¨ azure-ai-inference åº“è¿›è¡ŒAPIè°ƒç”¨
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        import re
        # åŒ¹é…å½¢å¦‚ https://xxx.services.ai.azure.com/models/chat/completions?api-version=xxx çš„URL
        match = re.match(r'https://(.+?)\.services\.ai\.azure\.com(?:/models)?(?:/chat/completions)?(?:\?api-version=(.+))?', base_url)
        if match:
            # endpointéœ€è¦æ˜¯å½¢å¦‚ https://xxx.services.ai.azure.com/models çš„æ ¼å¼
            self.endpoint = f"https://{match.group(1)}.services.ai.azure.com/models"
            # å¦‚æœURLä¸­åŒ…å«api-versionå‚æ•°ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
            self.api_version = match.group(2) if match.group(2) else "2024-05-01-preview"
        else:
            raise ValueError("Invalid Azure AI base_url format. Expected format: https://<endpoint>.services.ai.azure.com/models/chat/completions?api-version=xxx")
        
        self.base_url = self.endpoint  # å­˜å‚¨å¤„ç†åçš„endpoint URL
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = ChatCompletionsClient(
            endpoint=self.endpoint,
            credential=AzureKeyCredential(self.api_key),
            model=self.model_name,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.complete(
                messages=[
                    SystemMessage("You are a helpful assistant."),
                    UserMessage(prompt)
                ]
            )
            if response and response.choices:
                return response.choices[0].message.content
            else:
                logging.warning("No response from AzureAIAdapter.")
                return ""
        except Exception as e:
            logging.error(f"Azure AI Inference API è°ƒç”¨å¤±è´¥: {e}")
            return ""

# ç«å±±å¼•æ“å®ç°
class VolcanoEngineAIAdapter(BaseLLMAdapter):
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=timeout  # æ·»åŠ è¶…æ—¶é…ç½®
        )
    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯DeepSeekï¼Œæ˜¯ä¸€ä¸ª AI äººå·¥æ™ºèƒ½åŠ©æ‰‹"},
                    {"role": "user", "content": prompt},
                ],
                timeout=self.timeout  # æ·»åŠ è¶…æ—¶å‚æ•°
            )
            if not response:
                logging.warning("No response from DeepSeekAdapter.")
                return ""
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"ç«å±±å¼•æ“APIè°ƒç”¨è¶…æ—¶æˆ–å¤±è´¥: {e}")
            return ""

class SiliconFlowAdapter(BaseLLMAdapter):
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=timeout  # æ·»åŠ è¶…æ—¶é…ç½®
        )
    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯DeepSeekï¼Œæ˜¯ä¸€ä¸ª AI äººå·¥æ™ºèƒ½åŠ©æ‰‹"},
                    {"role": "user", "content": prompt},
                ],
                timeout=self.timeout  # æ·»åŠ è¶…æ—¶å‚æ•°
            )
            if not response:
                logging.warning("No response from DeepSeekAdapter.")
                return ""
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"ç¡…åŸºæµåŠ¨APIè°ƒç”¨è¶…æ—¶æˆ–å¤±è´¥: {e}")
            return ""
# grokå¯¦ç¾
class GrokAdapter(BaseLLMAdapter):
    """
    é€‚é… xAI Grok API
    """
    def __init__(self, api_key: str, base_url: str, model_name: str, max_tokens: int, temperature: float = 0.7, timeout: Optional[int] = 600):
        self.base_url = check_base_url(base_url)
        self.api_key = api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout

        self._client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key,
            timeout=self.timeout
        )

    def invoke(self, prompt: str) -> str:
        try:
            response = self._client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are Grok, created by xAI."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                timeout=self.timeout
            )
            if response and response.choices:
                return response.choices[0].message.content
            else:
                logging.warning("No response from GrokAdapter.")
                return ""
        except Exception as e:
            logging.error(f"Grok API è°ƒç”¨å¤±è´¥: {e}")
            return ""

def set_manual_proxy(http_proxy: str, https_proxy: str = None):
    """
    æ‰‹åŠ¨è®¾ç½®ä»£ç†
    
    Args:
        http_proxy: HTTPä»£ç†åœ°å€ï¼Œä¾‹å¦‚: "http://127.0.0.1:7890" æˆ– "socks5://127.0.0.1:7891"
        https_proxy: HTTPSä»£ç†åœ°å€ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨ä¸http_proxyç›¸åŒçš„å€¼
    """
    if https_proxy is None:
        https_proxy = http_proxy
    
    os.environ['HTTP_PROXY'] = http_proxy
    os.environ['HTTPS_PROXY'] = https_proxy
    os.environ['http_proxy'] = http_proxy  # å°å†™ç‰ˆæœ¬
    os.environ['https_proxy'] = https_proxy
    
    logging.info(f"æ‰‹åŠ¨è®¾ç½®ä»£ç†: HTTP={http_proxy}, HTTPS={https_proxy}")
    
    # æµ‹è¯•è¿æ¥
    if test_google_connectivity():
        logging.info("âœ… ä»£ç†è®¾ç½®æˆåŠŸï¼Œå¯ä»¥è®¿é—®GoogleæœåŠ¡")
        return True
    else:
        logging.warning("âš ï¸ ä»£ç†è®¾ç½®åä»æ— æ³•è®¿é—®GoogleæœåŠ¡")
        return False

def clear_proxy():
    """
    æ¸…é™¤ä»£ç†è®¾ç½®
    """
    proxy_vars = ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']
    for var in proxy_vars:
        if var in os.environ:
            del os.environ[var]
    logging.info("å·²æ¸…é™¤ä»£ç†è®¾ç½®")

def test_gemini_connection(api_key: str):
    """
    æµ‹è¯•Gemini APIè¿æ¥
    
    Args:
        api_key: Gemini APIå¯†é’¥
        
    Returns:
        bool: è¿æ¥æ˜¯å¦æˆåŠŸ
    """
    try:
        logging.info("ğŸ§ª æ­£åœ¨æµ‹è¯•Gemini APIè¿æ¥...")
        
        # å…ˆæµ‹è¯•ç½‘ç»œè¿æ¥
        if not test_google_connectivity():
            logging.warning("âŒ æ— æ³•è®¿é—®GoogleæœåŠ¡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä»£ç†è®¾ç½®")
            return False
            
        # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„APIè°ƒç”¨
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-pro')
        response = model.generate_content("Hello")
        
        if response and response.text:
            logging.info("âœ… Gemini APIè¿æ¥æˆåŠŸï¼")
            return True
        else:
            logging.warning("âš ï¸ Gemini APIå“åº”ä¸ºç©º")
            return False
            
    except Exception as e:
        logging.error(f"âŒ Gemini APIè¿æ¥å¤±è´¥: {e}")
        
        # æä¾›ä¸€äº›è§£å†³å»ºè®®
        if "API_KEY" in str(e).upper():
            logging.info("ğŸ’¡ è¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
        elif any(keyword in str(e).lower() for keyword in ['connection', 'timeout', 'network']):
            logging.info("ğŸ’¡ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå»ºè®®:")
            logging.info("   1. ç¡®ä¿Clashç­‰ä»£ç†æ­£å¸¸è¿è¡Œ")
            logging.info("   2. å°è¯•è¿è¡Œ: set_manual_proxy('http://127.0.0.1:7890')")
            logging.info("   3. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")
        
        return False

def create_llm_adapter(
    interface_format: str,
    base_url: str,
    model_name: str,
    api_key: str,
    temperature: float,
    max_tokens: int,
    timeout: int
) -> BaseLLMAdapter:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ® interface_format è¿”å›ä¸åŒçš„é€‚é…å™¨å®ä¾‹ã€‚
    """
    fmt = interface_format.strip().lower()
    if fmt == "deepseek":
        return DeepSeekAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "openai":
        return OpenAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "azure openai":
        return AzureOpenAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "azure ai":
        return AzureAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "ollama":
        return OllamaAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "ml studio":
        return MLStudioAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "gemini":
        return GeminiAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "é˜¿é‡Œäº‘ç™¾ç‚¼":
        return OpenAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "ç«å±±å¼•æ“":
        return VolcanoEngineAIAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "ç¡…åŸºæµåŠ¨":
        return SiliconFlowAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    elif fmt == "grok":
        return GrokAdapter(api_key, base_url, model_name, max_tokens, temperature, timeout)
    else:
        raise ValueError(f"Unknown interface_format: {interface_format}")
